{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Warehouse_block</th>\n",
       "      <th>Mode_of_Shipment</th>\n",
       "      <th>Customer_care_calls</th>\n",
       "      <th>Customer_rating</th>\n",
       "      <th>Cost_of_the_Product</th>\n",
       "      <th>Prior_purchases</th>\n",
       "      <th>Product_importance</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Discount_offered</th>\n",
       "      <th>Weight_in_gms</th>\n",
       "      <th>Reached</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>Flight</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>low</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>1233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>Flight</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>low</td>\n",
       "      <td>M</td>\n",
       "      <td>59</td>\n",
       "      <td>3088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>Flight</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>4</td>\n",
       "      <td>low</td>\n",
       "      <td>M</td>\n",
       "      <td>48</td>\n",
       "      <td>3374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>Flight</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "      <td>medium</td>\n",
       "      <td>M</td>\n",
       "      <td>10</td>\n",
       "      <td>1177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>Flight</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>3</td>\n",
       "      <td>medium</td>\n",
       "      <td>F</td>\n",
       "      <td>46</td>\n",
       "      <td>2484</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Warehouse_block Mode_of_Shipment  Customer_care_calls  Customer_rating  \\\n",
       "0               D           Flight                    4                2   \n",
       "1               F           Flight                    4                5   \n",
       "2               A           Flight                    2                2   \n",
       "3               B           Flight                    3                3   \n",
       "4               C           Flight                    2                2   \n",
       "\n",
       "   Cost_of_the_Product  Prior_purchases Product_importance Gender  \\\n",
       "0                  177                3                low      F   \n",
       "1                  216                2                low      M   \n",
       "2                  183                4                low      M   \n",
       "3                  176                4             medium      M   \n",
       "4                  184                3             medium      F   \n",
       "\n",
       "   Discount_offered  Weight_in_gms  Reached  \n",
       "0                44           1233        1  \n",
       "1                59           3088        1  \n",
       "2                48           3374        1  \n",
       "3                10           1177        1  \n",
       "4                46           2484        1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.rename(columns = {'Reached.on.Time_Y.N' : 'Reached'}, inplace = True)\n",
    "df = df.drop('ID', axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_care_calls</th>\n",
       "      <th>Customer_rating</th>\n",
       "      <th>Cost_of_the_Product</th>\n",
       "      <th>Prior_purchases</th>\n",
       "      <th>Product_importance</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Discount_offered</th>\n",
       "      <th>Weight_in_gms</th>\n",
       "      <th>Reached</th>\n",
       "      <th>Warehouse_block_A</th>\n",
       "      <th>Warehouse_block_B</th>\n",
       "      <th>Warehouse_block_C</th>\n",
       "      <th>Warehouse_block_D</th>\n",
       "      <th>Mode_of_Shipment_Flight</th>\n",
       "      <th>Mode_of_Shipment_Road</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>216</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>3088</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>3374</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1177</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2484</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10994</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>252</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1538</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10995</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>232</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1247</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>242</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10997</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>223</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1210</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1639</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10999 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Customer_care_calls  Customer_rating  Cost_of_the_Product  \\\n",
       "0                        4                2                  177   \n",
       "1                        4                5                  216   \n",
       "2                        2                2                  183   \n",
       "3                        3                3                  176   \n",
       "4                        2                2                  184   \n",
       "...                    ...              ...                  ...   \n",
       "10994                    4                1                  252   \n",
       "10995                    4                1                  232   \n",
       "10996                    5                4                  242   \n",
       "10997                    5                2                  223   \n",
       "10998                    2                5                  155   \n",
       "\n",
       "       Prior_purchases  Product_importance  Gender  Discount_offered  \\\n",
       "0                    3                   1       0                44   \n",
       "1                    2                   1       1                59   \n",
       "2                    4                   1       1                48   \n",
       "3                    4                   2       1                10   \n",
       "4                    3                   2       0                46   \n",
       "...                ...                 ...     ...               ...   \n",
       "10994                5                   2       0                 1   \n",
       "10995                5                   2       0                 6   \n",
       "10996                5                   1       0                 4   \n",
       "10997                6                   2       1                 2   \n",
       "10998                5                   1       0                 6   \n",
       "\n",
       "       Weight_in_gms  Reached  Warehouse_block_A  Warehouse_block_B  \\\n",
       "0               1233        1                  0                  0   \n",
       "1               3088        1                  0                  0   \n",
       "2               3374        1                  1                  0   \n",
       "3               1177        1                  0                  1   \n",
       "4               2484        1                  0                  0   \n",
       "...              ...      ...                ...                ...   \n",
       "10994           1538        1                  1                  0   \n",
       "10995           1247        0                  0                  1   \n",
       "10996           1155        0                  0                  0   \n",
       "10997           1210        0                  0                  0   \n",
       "10998           1639        0                  0                  0   \n",
       "\n",
       "       Warehouse_block_C  Warehouse_block_D  Mode_of_Shipment_Flight  \\\n",
       "0                      0                  1                        1   \n",
       "1                      0                  0                        1   \n",
       "2                      0                  0                        1   \n",
       "3                      0                  0                        1   \n",
       "4                      1                  0                        1   \n",
       "...                  ...                ...                      ...   \n",
       "10994                  0                  0                        0   \n",
       "10995                  0                  0                        0   \n",
       "10996                  1                  0                        0   \n",
       "10997                  0                  0                        0   \n",
       "10998                  0                  1                        0   \n",
       "\n",
       "       Mode_of_Shipment_Road  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "...                      ...  \n",
       "10994                      0  \n",
       "10995                      0  \n",
       "10996                      0  \n",
       "10997                      0  \n",
       "10998                      0  \n",
       "\n",
       "[10999 rows x 15 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.get_dummies(df, columns=['Warehouse_block', 'Mode_of_Shipment'], dtype = int)\n",
    "df1.drop(['Mode_of_Shipment_Ship', 'Warehouse_block_F'], axis = 1, inplace = True)\n",
    "df1['Product_importance'] = df1['Product_importance'].apply(lambda x : 3 if x == 'high' else (2 if x == 'medium' else 1))\n",
    "df1['Gender'] = df1['Gender'].apply(lambda x : 1 if x == 'M' else 0)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv('Encoded.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Logistic Regression F1 Score: 0.6246600181323663\n",
      "Logistic Regression Confusion Matrix:\n",
      "[[1109  224]\n",
      " [ 604  689]]\n",
      "------------------------------\n",
      "SVM F1 Score: 0.6356011183597391\n",
      "SVM Confusion Matrix:\n",
      "[[1162  171]\n",
      " [ 611  682]]\n",
      "------------------------------\n",
      "Naive Bayes F1 Score: 0.5945945945945945\n",
      "Naive Bayes Confusion Matrix:\n",
      "[[1326    7]\n",
      " [ 743  550]]\n",
      "------------------------------\n",
      "Decision Tree F1 Score: 0.7243027888446215\n",
      "Decision Tree Confusion Matrix:\n",
      "[[1025  308]\n",
      " [ 384  909]]\n",
      "------------------------------\n",
      "Random Forest F1 Score: 0.6971480307831598\n",
      "Random Forest Confusion Matrix:\n",
      "[[1187  146]\n",
      " [ 523  770]]\n",
      "------------------------------\n",
      "XGBoost F1 Score: 0.6900844819919965\n",
      "XGBoost Confusion Matrix:\n",
      "[[1153  180]\n",
      " [ 517  776]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def train_logistic_regression(self):\n",
    "        param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "        lr = LogisticRegression(solver='liblinear')\n",
    "        grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        self.lr_model = grid_search.best_estimator_\n",
    "        self.lr_f1 = f1_score(self.y_test, self.lr_model.predict(self.X_test_scaled))\n",
    "        self.lr_confusion_matrix = confusion_matrix(self.y_test, self.lr_model.predict(self.X_test_scaled))\n",
    "\n",
    "    def train_svm(self):\n",
    "        param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "        svm = SVC()\n",
    "        grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        self.svm_model = grid_search.best_estimator_\n",
    "        self.svm_f1 = f1_score(self.y_test, self.svm_model.predict(self.X_test_scaled))\n",
    "        self.svm_confusion_matrix = confusion_matrix(self.y_test, self.svm_model.predict(self.X_test_scaled))\n",
    "\n",
    "    def train_naive_bayes(self):\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(self.X_train_scaled, self.y_train)\n",
    "        self.nb_model = nb\n",
    "        self.nb_f1 = f1_score(self.y_test, self.nb_model.predict(self.X_test_scaled))\n",
    "        self.nb_confusion_matrix = confusion_matrix(self.y_test, self.nb_model.predict(self.X_test_scaled))\n",
    "\n",
    "    def train_decision_tree(self):\n",
    "        param_grid = {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "        dt = DecisionTreeClassifier()\n",
    "        grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        self.dt_model = grid_search.best_estimator_\n",
    "        self.dt_f1 = f1_score(self.y_test, self.dt_model.predict(self.X_test_scaled))\n",
    "        self.dt_confusion_matrix = confusion_matrix(self.y_test, self.dt_model.predict(self.X_test_scaled))\n",
    "\n",
    "    def train_random_forest(self):\n",
    "        param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]}\n",
    "        rf = RandomForestClassifier()\n",
    "        grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        self.rf_model = grid_search.best_estimator_\n",
    "        self.rf_f1 = f1_score(self.y_test, self.rf_model.predict(self.X_test_scaled))\n",
    "        self.rf_confusion_matrix = confusion_matrix(self.y_test, self.rf_model.predict(self.X_test_scaled))\n",
    "\n",
    "    def train_xgboost(self):\n",
    "        param_grid = {'n_estimators': [100, 200, 300], 'max_depth': [3, 4, 5], 'learning_rate': [0.01, 0.1, 0.2]}\n",
    "        xgb = XGBClassifier()\n",
    "        grid_search = GridSearchCV(xgb, param_grid, cv=5, scoring='f1')\n",
    "        grid_search.fit(self.X_train_scaled, self.y_train)\n",
    "        self.xgb_model = grid_search.best_estimator_\n",
    "        self.xgb_f1 = f1_score(self.y_test, self.xgb_model.predict(self.X_test_scaled))\n",
    "        self.xgb_confusion_matrix = confusion_matrix(self.y_test, self.xgb_model.predict(self.X_test_scaled))\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('../Data/Random_sampled.csv')\n",
    "    X = df.drop('Reached', axis=1)\n",
    "    y = df['Reached']\n",
    "    trainer = ModelTrainer(X, y)\n",
    "\n",
    "    print('-' * 30)\n",
    "    trainer.train_logistic_regression()\n",
    "    print(\"Logistic Regression F1 Score:\", trainer.lr_f1)\n",
    "    print(\"Logistic Regression Confusion Matrix:\")\n",
    "    print(trainer.lr_confusion_matrix)\n",
    "\n",
    "    print('-' * 30)\n",
    "    trainer.train_svm()\n",
    "    print(\"SVM F1 Score:\", trainer.svm_f1)\n",
    "    print(\"SVM Confusion Matrix:\")\n",
    "    print(trainer.svm_confusion_matrix)\n",
    "\n",
    "    print('-' * 30)\n",
    "    trainer.train_naive_bayes()\n",
    "    print(\"Naive Bayes F1 Score:\", trainer.nb_f1)\n",
    "    print(\"Naive Bayes Confusion Matrix:\")\n",
    "    print(trainer.nb_confusion_matrix)\n",
    "\n",
    "    print('-' * 30)\n",
    "    trainer.train_decision_tree()\n",
    "    print(\"Decision Tree F1 Score:\", trainer.dt_f1)\n",
    "    print(\"Decision Tree Confusion Matrix:\")\n",
    "    print(trainer.dt_confusion_matrix)\n",
    "\n",
    "    print('-' * 30)\n",
    "    trainer.train_random_forest()\n",
    "    print(\"Random Forest F1 Score:\", trainer.rf_f1)\n",
    "    print(\"Random Forest Confusion Matrix:\")\n",
    "    print(trainer.rf_confusion_matrix)\n",
    "\n",
    "    print('-' * 30)\n",
    "    trainer.train_xgboost()\n",
    "    print(\"XGBoost F1 Score:\", trainer.xgb_f1)\n",
    "    print(\"XGBoost Confusion Matrix:\")\n",
    "    print(trainer.xgb_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "230 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "72 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "158 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan 0.68624705 0.67905134\n",
      "        nan        nan 0.68324123        nan        nan 0.68349425\n",
      " 0.67928286 0.68127933 0.68597946 0.67856674        nan        nan\n",
      "        nan 0.68485205        nan 0.67730345 0.67977571        nan\n",
      " 0.67966004 0.68384009 0.68286959 0.66632647        nan        nan\n",
      "        nan 0.66780793        nan        nan        nan        nan\n",
      " 0.68472081        nan        nan 0.66744623        nan        nan\n",
      "        nan        nan        nan        nan 0.68550547 0.68469917\n",
      " 0.68685438 0.68034078        nan 0.67968277        nan 0.6660788\n",
      " 0.69062315 0.68084436 0.68037763 0.68153198 0.68232823        nan\n",
      " 0.68550547        nan 0.68505076 0.66473955        nan 0.67883321\n",
      "        nan 0.67971388 0.68610726        nan 0.68102289 0.68153198\n",
      " 0.67988463 0.67790056        nan        nan 0.68084774        nan\n",
      "        nan 0.68564037 0.6826081         nan        nan 0.67945892\n",
      " 0.67898353 0.68060516        nan        nan 0.67905946 0.69299443\n",
      " 0.68153198 0.68068819        nan 0.6866846  0.68356946 0.68446059\n",
      " 0.66591175        nan        nan 0.67883321]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest F1 Score: 0.6944894651539709\n",
      "Best Random Forest Parameters: {'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 80, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 700, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer_1:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "\n",
    "    def train_random_forest(self):\n",
    "        param_dist = {\n",
    "            'n_estimators': np.arange(100, 1000, 100),\n",
    "            'max_features': ['auto', 'sqrt'],\n",
    "            'max_depth': [None] + list(np.arange(10, 110, 10)),\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "        rf = RandomForestClassifier(random_state=42)\n",
    "        random_search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=100, cv=5, scoring='f1', n_jobs=-1, random_state=42)\n",
    "        random_search.fit(self.X_train_scaled, self.y_train)\n",
    "        self.rf_model = random_search.best_estimator_\n",
    "        self.rf_f1 = f1_score(self.y_test, self.rf_model.predict(self.X_test_scaled))\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainer = ModelTrainer_1(X, y)\n",
    "    trainer.train_random_forest()\n",
    "\n",
    "    print(\"Random Forest F1 Score:\", trainer.rf_f1)\n",
    "    print(\"Best Random Forest Parameters:\", trainer.rf_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Best F1 Score: 0.7246 - Best Hyperparameters: {'C': 0.001, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "SVM - Best F1 Score: 0.7447 - Best Hyperparameters: {'C': 0.001, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "Naive Bayes - Best F1 Score: 0.5993 - Best Hyperparameters: {'priors': None, 'var_smoothing': 1e-09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 1 is smaller than n_iter=10. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Best F1 Score: 0.6959 - Best Hyperparameters: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 50, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': None, 'splitter': 'best'}\n",
      "Random Forest - Best F1 Score: 0.6961 - Best Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 50, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "XGBoost - Best F1 Score: 0.7130 - Best Hyperparameters: {'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.2, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 4, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 400, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': None, 'subsample': 0.8, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n",
      "Best Model: SVM - Best F1 Score: 0.7130\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, param_grid):\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.best_model = None\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        grid_search = RandomizedSearchCV(self.model, self.param_grid, scoring='f1', cv=5, n_iter=10, random_state=42, error_score='raise')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        return self.best_model\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.best_model.predict(X_test)\n",
    "\n",
    "def main(X,y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define models and hyperparameter grids\n",
    "    models = {\n",
    "        'Logistic Regression': (LogisticRegression(),\n",
    "                                {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}),\n",
    "        \n",
    "        'SVM': (SVC(),\n",
    "                {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'],\n",
    "                 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]}),\n",
    "\n",
    "        'Naive Bayes': (GaussianNB(), {}),\n",
    "\n",
    "        'Decision Tree': (DecisionTreeClassifier(),\n",
    "                          {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "                           'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}),\n",
    "\n",
    "        'Random Forest': (RandomForestClassifier(),\n",
    "                          {'n_estimators': [50, 100, 200, 400], 'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "                           'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4]}),\n",
    "\n",
    "        'XGBoost': (xgb.XGBClassifier(),\n",
    "                    {'n_estimators': [50, 100, 200, 400], 'max_depth': [3, 4, 5, 6],\n",
    "                     'learning_rate': [0.01, 0.1, 0.2], 'subsample': [0.8, 0.9, 1.0]})\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    \n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        trainer = ModelTrainer(model, param_grid)\n",
    "        best_model = trainer.train(X_train_scaled, y_train)\n",
    "        y_pred = trainer.predict(X_test_scaled)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        print(f'{model_name} - Best F1 Score: {f1:.4f} - Best Hyperparameters: {trainer.best_model.get_params()}')\n",
    "        \n",
    "        best_models[model_name] = trainer.best_model\n",
    "    \n",
    "    best_model_name = max(best_models, key=lambda k: f1_score(y_test, best_models[k].predict(X_test_scaled)))\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    print(f'Best Model: {best_model_name} - Best F1 Score: {f1:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningModel(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_dim, hidden_units=128, activation='relu'):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.model = Sequential()\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model.add(Dense(self.hidden_units, input_dim=self.input_dim, activation=self.activation))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, X, y, epochs=10, batch_size=32):\n",
    "        self.model.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(int)\n",
    "\n",
    "# Define an AdaBoostClassifier with the deep learning model\n",
    "class AdaBoostDLClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator=None, n_estimators=50, learning_rate=1.0):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        base_estimator = self.base_estimator or DeepLearningModel(input_dim=X.shape[1])\n",
    "        self.model = AdaBoostClassifier(base_estimator=base_estimator, n_estimators=self.n_estimators, learning_rate=self.learning_rate)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "# Hyperparameter optimization using GridSearchCV\n",
    "def optimize_hyperparameters(X, y):\n",
    "    param_grid = {\n",
    "        'base_estimator__base_estimator__hidden_units': [64, 128],\n",
    "        'base_estimator__base_estimator__activation': ['relu', 'tanh'],\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.1, 0.5]\n",
    "    }\n",
    "\n",
    "    ada_boost_model = AdaBoostDLClassifier()\n",
    "\n",
    "    grid_search = GridSearchCV(ada_boost_model, param_grid, cv=3, scoring='f1')\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "# Train and evaluate the model\n",
    "best_params, best_score = optimize_hyperparameters(X_train, y_train)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best F1 Score:\", best_score)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "final_model = AdaBoostDLClassifier(base_estimator=DeepLearningModel(input_dim=X_train.shape[1], **best_params))\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "y_pred = final_model.predict(X_test)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Final F1 Score on Test Data:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:01<01:27,  1.79s/trial, best loss: -0.6613636612892151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:03<01:14,  1.56s/trial, best loss: -0.6613636612892151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:04<00:59,  1.27s/trial, best loss: -0.6613636612892151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:06<01:18,  1.70s/trial, best loss: -0.6613636612892151]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:07<01:09,  1.55s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:09<01:05,  1.50s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [00:10<01:06,  1.54s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:12<01:01,  1.45s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:13<01:00,  1.47s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:14<00:50,  1.27s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:15<00:47,  1.22s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:18<01:10,  1.86s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [00:20<01:05,  1.77s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:23<01:23,  2.31s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:26<01:22,  2.35s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:28<01:21,  2.41s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [00:30<01:14,  2.26s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:32<01:04,  2.02s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [00:34<01:01,  1.98s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:35<00:53,  1.78s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [00:39<01:12,  2.51s/trial, best loss: -0.6677272915840149]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:41<01:00,  2.15s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [00:42<00:51,  1.91s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:43<00:44,  1.72s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [00:44<00:39,  1.59s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:46<00:35,  1.49s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [00:47<00:34,  1.51s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [00:49<00:31,  1.45s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:49<00:25,  1.21s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:50<00:24,  1.23s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [00:51<00:19,  1.05s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [00:52<00:20,  1.13s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [00:54<00:20,  1.18s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [00:55<00:19,  1.22s/trial, best loss: -0.6700000166893005]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:56<00:18,  1.25s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:57<00:14,  1.06s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [00:58<00:15,  1.16s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [01:00<00:16,  1.35s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [01:01<00:14,  1.33s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [01:02<00:12,  1.21s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [01:03<00:10,  1.11s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [01:05<00:09,  1.18s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [01:07<00:10,  1.44s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [01:09<00:10,  1.72s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [01:10<00:07,  1.41s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [01:12<00:06,  1.67s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [01:14<00:05,  1.74s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [01:15<00:03,  1.63s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [01:17<00:01,  1.52s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.56s/trial, best loss: -0.6759091019630432]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameters:\n",
      "{'batch_size': 0, 'dropout_rate': 0.43129036086834394, 'epochs': 0, 'hidden_units': 0, 'learning_rate': 0.008138714482545035}\n",
      "69/69 [==============================] - 0s 381us/step - loss: 0.6931 - accuracy: 0.4068\n",
      "Test Accuracy: 40.68%\n"
     ]
    }
   ],
   "source": [
    "space = {\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),\n",
    "    'dropout_rate': hp.uniform('dropout_rate', 0.2, 0.5),\n",
    "    'hidden_units': hp.choice('hidden_units', [64, 128, 256]),\n",
    "    'epochs': hp.choice('epochs', [10, 20, 30]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128])\n",
    "}\n",
    "\n",
    "# Define the deep learning model\n",
    "def create_model(params):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        keras.layers.Dense(params['hidden_units'], activation='relu'),\n",
    "        keras.layers.Dropout(params['dropout_rate']),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the objective function to minimize (cross-validation)\n",
    "def objective(params):\n",
    "    model = create_model(params)\n",
    "    history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "    _, val_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return -val_accuracy  # Negative accuracy to maximize\n",
    "\n",
    "# Hyperparameter optimization using Hyperopt\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, verbose=1, rstate=np.random.seed(42))\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_model = create_model(best)\n",
    "best_model.fit(X_train, y_train, epochs=best['epochs'], batch_size=best['batch_size'], verbose=1)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.6695454545454546\n"
     ]
    }
   ],
   "source": [
    "svm_model = SVC()\n",
    "\n",
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],           # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],      # Kernel type (linear or radial basis function)\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]  # Kernel coefficient (for 'rbf' kernel)\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for hyperparameter optimization\n",
    "random_search = RandomizedSearchCV(estimator=svm_model, param_distributions=param_grid, n_iter=10, scoring='accuracy', cv=5, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Train a new SVM model with the best hyperparameters\n",
    "best_svm_model = SVC(**best_params)\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on the test set:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rachitkumarsingh/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:307: UserWarning: The total space of parameters 6 is smaller than n_iter=10. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Best F1 Score: 0.7246 - Best Hyperparameters: {'C': 0.001, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
      "SVM - Best F1 Score: 0.7447 - Best Hyperparameters: {'C': 0.001, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.01, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n",
      "Best Model: SVM - Best F1 Score: 0.7447\n"
     ]
    }
   ],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model, param_grid):\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.best_model = None\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        grid_search = RandomizedSearchCV(self.model, self.param_grid, scoring='f1', cv=5, n_iter=10, random_state=42, error_score='raise')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        return self.best_model\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.best_model.predict(X_test)\n",
    "\n",
    "def main(X,y):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Define models and hyperparameter grids\n",
    "    models = {\n",
    "        'Logistic Regression': (LogisticRegression(),\n",
    "                                {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}),\n",
    "        \n",
    "        'SVM': (SVC(),\n",
    "                {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'],\n",
    "                 'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]})\n",
    "    }\n",
    "\n",
    "    best_models = {}\n",
    "    \n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        trainer = ModelTrainer(model, param_grid)\n",
    "        best_model = trainer.train(X_train_scaled, y_train)\n",
    "        y_pred = trainer.predict(X_test_scaled)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        print(f'{model_name} - Best F1 Score: {f1:.4f} - Best Hyperparameters: {trainer.best_model.get_params()}')\n",
    "        \n",
    "        best_models[model_name] = trainer.best_model\n",
    "    \n",
    "    best_model_name = max(best_models, key=lambda k: f1_score(y_test, best_models[k].predict(X_test_scaled)))\n",
    "    best_model = best_models[best_model_name]\n",
    "    \n",
    "    print(f'Best Model: {best_model_name} - Best F1 Score: {f1:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
